<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Case Study: MapReduce 2004 Consistency and Linearizability Case Study: GFS 2003 Primary&#x2F;Backup Replication Case Study: VMware FT 2010  FT design Failure Handling Design Alternatives   Verified Prim">
<meta property="og:type" content="article">
<meta property="og:title" content="Distributed System Notes">
<meta property="og:url" content="https://sgzerolc.github.io/sg/2023/07/28/ds_basics/index.html">
<meta property="og:site_name" content="steins gate zero">
<meta property="og:description" content="Case Study: MapReduce 2004 Consistency and Linearizability Case Study: GFS 2003 Primary&#x2F;Backup Replication Case Study: VMware FT 2010  FT design Failure Handling Design Alternatives   Verified Prim">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://sgzerolc.github.io/sg/2023/07/28/ds_basics/zab.png">
<meta property="article:published_time" content="2023-07-27T22:00:00.000Z">
<meta property="article:modified_time" content="2025-08-27T16:42:38.449Z">
<meta property="article:author" content="Sam Li">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sgzerolc.github.io/sg/2023/07/28/ds_basics/zab.png">
    
    
      
        
          <link rel="shortcut icon" href="/sg/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/sg/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/sg/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>Distributed System Notes</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/sg/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/sg/">Home</a></li><!--
     --><!--
       --><li><a href="/sg/about/">About</a></li><!--
     --><!--
       --><li><a href="/sg/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/sg/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/sg/CV/">CV</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/sg/2023/08/03/SD_network_layer/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/sg/2023/06/27/summer/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&text=Distributed System Notes"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&title=Distributed System Notes"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&is_video=false&description=Distributed System Notes"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Distributed System Notes&body=Check out this article: https://sgzerolc.github.io/sg/2023/07/28/ds_basics/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&title=Distributed System Notes"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&title=Distributed System Notes"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&title=Distributed System Notes"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&title=Distributed System Notes"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&name=Distributed System Notes&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&t=Distributed System Notes"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Case Study: MapReduce 2004</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Consistency and Linearizability</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">Case Study: GFS 2003</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">Primary&#x2F;Backup Replication</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">Case Study: VMware FT 2010</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">5.1.</span> <span class="toc-text">FT design</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">5.2.</span> <span class="toc-text">Failure Handling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">5.3.</span> <span class="toc-text">Design Alternatives</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">Verified Primary&#x2F;Backup</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">Case Study: Grove 2023</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">8.</span> <span class="toc-text">Raft</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.1.</span> <span class="toc-text">Raft Review</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.2.</span> <span class="toc-text">Leader Election</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.3.</span> <span class="toc-text">Log</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.4.</span> <span class="toc-text">Persistence</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.5.</span> <span class="toc-text">Log Compaction and Snapshots</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.6.</span> <span class="toc-text">Linearizability</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.7.</span> <span class="toc-text">Duplicate RPC Detection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.8.</span> <span class="toc-text">Read-only Operations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">9.</span> <span class="toc-text">Paxos</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">9.1.</span> <span class="toc-text">Paxos Review</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">9.2.</span> <span class="toc-text">Inference</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">10.</span> <span class="toc-text">*Case Study: ZooKeeper 2010</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">11.</span> <span class="toc-text">*Case Study: Chubby 2006</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">12.</span> <span class="toc-text">*Case Study: Harp 1991</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Distributed System Notes
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Sam Li</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2023-07-27T22:00:00.000Z" itemprop="datePublished">2023-07-28</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/sg/categories/computer-systems/">computer systems</a> › <a class="category-link" href="/sg/categories/computer-systems/6-824-distributed-systems/">6.824 distributed systems</a>
    </div>


      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <div class="toc">
<!-- toc -->
<ul>
<li><a href="#case-study-mapreduce-2004">Case Study: MapReduce 2004</a></li>
<li><a href="#consistency-and-linearizability">Consistency and Linearizability</a></li>
<li><a href="#case-study-gfs-2003">Case Study: GFS 2003</a></li>
<li><a href="#primary-backup-replication">Primary/Backup Replication</a></li>
<li><a href="#case-study-vmware-ft-2010">Case Study: VMware FT 2010</a>
<ul>
<li><a href="#ft-design">FT design</a></li>
<li><a href="#failure-handling">Failure Handling</a></li>
<li><a href="#design-alternatives">Design Alternatives</a></li>
</ul>
</li>
<li><a href="#verified-primary-backup">Verified Primary/Backup</a></li>
<li><a href="#case-study-grove-2023">Case Study: Grove 2023</a></li>
<li><a href="#raft">Raft</a>
<ul>
<li><a href="#raft-review">Raft Review</a></li>
<li><a href="#leader-election">Leader Election</a></li>
<li><a href="#log">Log</a></li>
<li><a href="#persistence">Persistence</a></li>
<li><a href="#log-compaction-and-snapshots">Log Compaction and Snapshots</a></li>
<li><a href="#linearizability">Linearizability</a></li>
<li><a href="#duplicate-rpc-detection">Duplicate RPC Detection</a></li>
<li><a href="#read-only-operations">Read-only Operations</a></li>
</ul>
</li>
<li><a href="#paxos">Paxos</a>
<ul>
<li><a href="#paxos-review">Paxos Review</a></li>
<li><a href="#inference">Inference</a></li>
</ul>
</li>
<li><a href="#case-study-zookeeper-2010">*Case Study: ZooKeeper 2010</a></li>
<li><a href="#case-study-chubby-2006">*Case Study: Chubby 2006</a></li>
<li><a href="#case-study-harp-1991">*Case Study: Harp 1991</a></li>
</ul>
<!-- tocstop -->
</div>
<p>Design goals</p>
<ul>
<li>
<p>Infrastructure services: storage, communication, computing. The course goal is to build an abstraction that hides the distribution while realizing high performance.</p>
</li>
<li>
<p>performance -&gt; scalability</p>
<ul>
<li>scenario: n users &lt;=&gt; 1 web server &lt;=&gt; 1 database</li>
<li>When n is 1million, scaling up web servers and spliting users accounts into multiple webs can achieve performance until web-db communication becomes bottle neck, that is, scalable web servers no longer help. Then db will need to be refactored so that is splitting data into multiple dbs.</li>
<li>terms
<ul>
<li><code>scalability</code>: N times computers/resources can achieve N times throughput.</li>
</ul>
</li>
<li>special cases
<ul>
<li>quick response time for a single user request</li>
<li>all users want to update the same data</li>
</ul>
</li>
</ul>
</li>
<li>
<p>fault tolerance -&gt; availability, recoveribility</p>
<ul>
<li>
<p>scenario: always sth brokes when running 1000 computers vs running 1</p>
</li>
<li>
<p>rare events become real problems</p>
</li>
<li>
<p>terms</p>
<ul>
<li>
<p><code>availability</code>: system will keep operating when certain kinds of failures occur. It will not be available when too many failures occur.</p>
</li>
<li>
<p><code>recoverability</code>: system stops running/reponding to requests until the failed componets are repaired. Need to save latest data on disk etc so they can recover it when powering up.</p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/1371400/how-much-faster-is-the-memory-usually-than-the-disk">memory access speed V.S. disk</a></p>
</li>
<li>
<p>normal scale: 3 GHz microprocessor</p>
</li>
</ul>
</li>
<li>
<p>tools</p>
<ul>
<li>non-volatile storage: store log of states. -&gt; writing NV storage is too slow (move disk arms and wait for a disk platter to rotate)</li>
<li>replicated servers: use a copy -&gt; two replicas drifted out of sync</li>
</ul>
</li>
</ul>
</li>
<li>
<p>consistency</p>
<ul>
<li>
<blockquote>
<p>General-purpose infrastructure needs well-defined behavior.</p>
</blockquote>
</li>
<li>
<p>everything could broke and lead to lose replica.</p>
</li>
<li>
<p>strong consistency often means poor performance relative to weaker consistency.</p>
</li>
</ul>
</li>
</ul>
<h2><span id="case-study-mapreduce-2004">Case Study: MapReduce 2004</span><a href="#case-study-mapreduce-2004" class="header-anchor">¶</a></h2>
<h2><span id="consistency-and-linearizability">Consistency and Linearizability</span><a href="#consistency-and-linearizability" class="header-anchor">¶</a></h2>
<h2><span id="case-study-gfs-2003">Case Study: GFS 2003</span><a href="#case-study-gfs-2003" class="header-anchor">¶</a></h2>
<h2><span id="primary-backup-replication">Primary/Backup Replication</span><a href="#primary-backup-replication" class="header-anchor">¶</a></h2>
<ol>
<li>One way of providing fault tolerance: Replication.</li>
<li>Replication deals with “fail-stop” failure of a single replica. It may not be able to detect h/w or bugs in s/w or human config errors. Geo-failure are dealt with only if replicas are physically separated.</li>
<li>Failures in one server are independent from another otherwise it’s useless to use replication.</li>
<li>Whether replication is worthwhile the N*expense of computing resources depends on the consequences of failures.</li>
<li>Two main approaches:
<ul>
<li>state transfer: primary sends internal memory (state) to backup</li>
<li>replicated state machine: clients send operations to primary and primary sends those external events to backups. All replicas have deterministic same state as the primary if they execute the operations in the same order.</li>
</ul>
</li>
<li>tradeoffs:
<ul>
<li>state transfer is simpler but may be large and slow to transfer over network</li>
<li>replicated state machine are comparably smaller than state and generates less network traffic but complex to get right.</li>
</ul>
</li>
<li>example: VM-FT uses replicated state machine on single-core processor. It uses state transfer scheme when it is expanded to multi-core.</li>
<li>Design points of replication schema
<ul>
<li>What state to replicate?</li>
<li>Does primary have to wait for backup?</li>
<li>When to cut over to backup?</li>
<li>Are anomalies visible at cut-over?</li>
<li>How to bring a replacement backup up to speed?</li>
</ul>
</li>
</ol>
<p>Problem 1: At what level do we want replicas to be identical?</p>
<p>application state: forward op stream, can be efficient. ex: GFS.</p>
<p>machine level, registers and RAM content: forward machine events (interrupts, DMA, &amp;c), not as efficient. ex: run an existing software without modification</p>
<h2><span id="case-study-vmware-ft-2010">Case Study: VMware FT 2010</span><a href="#case-study-vmware-ft-2010" class="header-anchor">¶</a></h2>
<p>Terms:</p>
<ol>
<li>vm-ft: fault-tolerant virtual machines</li>
<li>failstop: if something goes wrong, the computer would stop executing instead of generating incorrect results. E.g. unplug the power cable out of the server, CPU overheats.</li>
<li>failover: if something goes wrong, the computer will be replaced by another one.</li>
<li>virtual lockstep</li>
<li>mode: logging, replaying, normal</li>
<li>a deferred-execution context (similar to a tasklet in Linux) is a mechanism for deferring work to be executed later at a more appropriate time</li>
</ol>
<h3><span id="ft-design">FT design</span><a href="#ft-design" class="header-anchor">¶</a></h3>
<ol>
<li>Synchronization of primary and backup VMs is based on the technique of deterministic replay. Briefly talking, deterministic replay transfers the non-deterministic operations/events to the log entries of the file. The backup can read from the file and replay execution of the primary.
<ul>
<li>non-determinism of events and operations in VMs: virtual interrupts, reading the clock cycle counter of the processor</li>
<li>question: non-deterministic input (capture and apply) -&gt; deterministic execution + performance unaffected (at instruction level)</li>
<li>However, that technique for VMware vSphere platform is introduced in 2007.</li>
</ul>
</li>
<li>FT protocol: No data is lost if a backup takes over after the primary fails.
<ul>
<li>Output requirement &lt;- output rule: The backup is supposed to execute consistently as the primary after it takes over the primary VM. The rule to guarantee that is two phase commit. The primary delays sending the output until the backup VM has received and acknowledged the log entries of the output operations.</li>
<li>Can tolerate lost packets: incoming packets may be dropped during failure of the primary due to reasons unrelated to that failure.</li>
</ul>
</li>
</ol>
<h3><span id="failure-handling">Failure Handling</span><a href="#failure-handling" class="header-anchor">¶</a></h3>
<ol>
<li>
<p>context: time lag on the execution of the backup VM. To control the time lag to be less than 100ms and no greater than 1s, the primary VM’s CPU limit is managed to increase/decrease to occupy/spare more execution time.</p>
</li>
<li>
<p>failure detection: use heartbeating (UDP) to monitor the traffic on the logging channel that connects primary and backup. A timeout in the flow of log entries and acknowledgements is detected as a failure.</p>
</li>
<li>
<p>split-brain problem: solution: (requirement) only one VM (primary or backup) takes over the execution.</p>
<blockquote class="colorquote blue"><p>Problem 0: How does VM-FT handle network partitions? That is, is it possible that if the primary and the backup end up in different network partitions that the backup will become a primary too and the system will run with two primaries?</p>
</blockquote>
<p>There will be only one VM executing during network partitions. The primary VM delays sending outputs to the external world until the backup receives and acknowledges the log entries sent by it. When network partitions occur, the primary and backup VMs lost information of each other. The failure whether caused by network connection or VM faults is unknown.</p>
<p>VM-FT hence performs an atomic op test-and-set on shared storage that stores the virtual disks of VMs when the primary or the backup wants to go live. If the op succeeds, then the VM is allowed to go live. If not, that means a VM must have been live, so the current VM halts itself. If the VM cannot access shared storage, it waits until it can.</p>
</li>
<li>
<p>Start a new backup on another host: (primary, backup) = (P1, B1) -&gt; (P1 is down, B1 takes over) -&gt; (B1, ?) -&gt; (B1, B2)</p>
<ul>
<li>Based on VMware VMotion: migration of a running VM from one server to another server within one second.</li>
<li>Modified FT VMotion clones a VM to a remote host rather than migrating it within minutes or unnoticeable interruption.</li>
<li>Simplified steps: the mode of B1, B2: (logging, replaying) -&gt; choose a server to run B2: cluster service decides it -&gt; cloning…</li>
</ul>
</li>
<li>
<p>To eliminate non-determinism</p>
<ul>
<li>
<p>on disk IO issues:</p>
<ul>
<li>parallel disk IOs try to access the same location on the shared storage -&gt; detect IO races and make it sequential</li>
<li>a disk op races with an application op when they are reading the same memory block at the same time -&gt;  MMU protection on pages - traps - (too expensive) =&gt; bounce buffers (cheaper) to read/write from/at</li>
<li>when failures happen, no way to find out the IOs issuing during that time are completed or not. -&gt; reissue the pending IOs</li>
</ul>
</li>
<li>
<p>network IO issues: async updates to a VM’s state while executing ?-&gt; VM traps and interrupts + delaying the sending packets-&gt; performance challenges</p>
</li>
</ul>
</li>
</ol>
<p>Aside: <s>Ignored: operations on VM-FT</s></p>
<h3><span id="design-alternatives">Design Alternatives</span><a href="#design-alternatives" class="header-anchor">¶</a></h3>
<p>non-shared storage vs shared storage</p>
<p>advantages:</p>
<ol>
<li>no delaying disk writes for the primary VM</li>
<li>adds availability when the primary and backup VMs are far apart</li>
</ol>
<p>disadvantages:</p>
<ol>
<li>need the alternative to handle network partition</li>
<li>need to sync disks of VMs</li>
</ol>
<h2><span id="verified-primary-backup">Verified Primary/Backup</span><a href="#verified-primary-backup" class="header-anchor">¶</a></h2>
<h2><span id="case-study-grove-2023">Case Study: Grove 2023</span><a href="#case-study-grove-2023" class="header-anchor">¶</a></h2>
<p>Grove: a Separation-Logic Library for Verifying Distributed Systems</p>
<h2><span id="raft">Raft</span><a href="#raft" class="header-anchor">¶</a></h2>
<p>site:</p>
<ol>
<li><a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2022/notes/l-raft.txt">http://nil.csail.mit.edu/6.824/2022/notes/l-raft.txt</a></li>
<li><a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2022/notes/l-raft2.txt">http://nil.csail.mit.edu/6.824/2022/notes/l-raft2.txt</a></li>
</ol>
<p>Problem 0: a machine doesn’t understand the real-life events like network broken, disk failures, server crashed. All it can notice is the state changes.</p>
<ul>
<li>It appears that no response to a query over the network.</li>
<li>Unless an outside agent will decide when to switch servers/a perfectly reliable network (never breaks)/ a single perfectly reliable server (vmware FT’s test-and-set server) -&gt; possible single-point failure</li>
<li>How to automate it?</li>
</ul>
<p>Main idea:</p>
<ul>
<li>majority rule: same as Paxos. At lease one server is in the intersection of any two intersect which can convey info about previous decisions. Such a system often is called quorum systems.</li>
<li>electing a new leader</li>
<li>Logs of each server can be different but they will converge to be identical despite failures.</li>
</ul>
<p>The crucial property:</p>
<ul>
<li>committed: an entry that is commited won’t be forgotten despite failures.</li>
<li>logs with <strong>ordered</strong> commands:
<ul>
<li>It stores the ordered commands in case leader must re-send to followers or for replay after reboot (persistence).</li>
<li>help to decide if the machines have identical logs</li>
<li>help replicas agree on a single execution order</li>
</ul>
</li>
<li>leader: leader ensures the identical execution order of the same commands on all servers.</li>
</ul>
<p>Raft provides:</p>
<ul>
<li>interface:
<ul>
<li>Start(command) (index, term, isleader): only leader can start proposals.</li>
<li>ApplyMsg(index, command): each peer sends this message for each committed entry to local service.</li>
</ul>
</li>
<li>more understandable than Paxos</li>
</ul>
<p>A little about history of consensus protocols: paxos (1998), viewstamped replication (1988, 2012). Raft (2014) is more modern. They are all partition-tolerant.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Raft diagram:</span><br><span class="line">Members: Client, Follower, Leader, denoted by C, F, L respectively.</span><br><span class="line"></span><br><span class="line">Client       ----        Leader          ----                    Follower exchange:</span><br><span class="line">        Put/Get(n1) -&gt;  Start(n1)</span><br><span class="line">                        [appendLog(n1)]  AppendEntries(n1) -&gt;</span><br><span class="line">                        [committed(n1)]  &lt;- reply(n1)</span><br><span class="line">                        [execute(c1)]</span><br><span class="line">        &lt;- reply(c1)</span><br><span class="line"></span><br><span class="line">        Put/Get(n2) -&gt;  [appendLog(n2)]  AppendEntries(c1, n2) -&gt;       ApplyMsg(c1)</span><br><span class="line">                        [committed(n2)]   &lt;- reply(n1)                  [execute(c1)]</span><br><span class="line">                        [execute(c2)]</span><br><span class="line">        &lt;- reply(c2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Problem 1: How Raft works?</p>
<h3><span id="raft-review">Raft Review</span><a href="#raft-review" class="header-anchor">¶</a></h3>
<ol>
<li>
<p>State machine safety: once a server has applied a log entry at a given index to its state machine, no other servers can apply a different log entry for the same index.</p>
<p>State machine safety is the key safety property of Raft.</p>
</li>
<li>
<p>Log matching property: a follower only accepts an append at index <code>i</code> if it already has the leader’s entry at <code>i-1</code> with the same term.</p>
</li>
<li>
<p>Raft determines a more up-to-date log by comparing the index and term of the last entries in the log. If the term is different, the log with a higher term is more up-to-date. If the term is the same, whichever log is longer is more up-to-date.</p>
</li>
</ol>
<p>Considering a case where a follower is unavailable while the leader commits several log enries and then is elected leader and overwrites these entries, how Raft solve this problem by puting constraints on candidates?</p>
<ol>
<li>A candidate can’t win an election unless its log has all commited entries.</li>
<li>Raft never commits log entries from previous terms by counting replicas.</li>
</ol>
<p>A leader may only use replica counting to commit entries from its current term. When it commits a current-term entry at index <code>N</code>, it atomically advances its commitIndex to <code>N</code>. When the current-term entry can be commited, it also implies all earler entries less than <code>N</code> with older terms become commited indirectly.</p>
<p>The prohibition is only against declaring an <em>older-term</em> entry “committed” merely because it is on a majority. You must first commit some current-term entry at or past that index. Then all prior uncommited entries will be commited.</p>
<p>Analysis of Figure 8d:</p>
<p>S1 was leader earlier and had an older-term entry at index 2 (say term 2) spread to a majority, but not safely committed. If S1 crashes before committing any current-term entry, S5 (whose log has a conflicting entry at index 2 from term 3) can win the next election and overwrite index 2 again.</p>
<p>Raft prevents case d by commit of a current-term entry, not just by whom elections are won. Once that current-term entry is committed:</p>
<ol>
<li><code>commitIndex</code> advances past it, so all earlier entries (including the old index 2) are committed indirectly.</li>
<li>Any future leader must contain that committed term 4 entry (RequestVote’s “up-to-date” rule + majority intersection), so a node like S5 can’t win unless it also has that entry, meaning it can’t overwrite index 2.</li>
</ol>
<p>If S1 were elected term 4 but crashed before committing any term-4 entry to a majority, then (d) could still happen: S5 might win and overwrite index 2.</p>
<h3><span id="leader-election">Leader Election</span><a href="#leader-election" class="header-anchor">¶</a></h3>
<p>Leader (srv, term): the term helps servers to follow latest leader.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Election diagram:</span><br><span class="line"></span><br><span class="line">Follower (S0, term+1) -&gt; Candidate (S0, term+1)  -&gt;  Leader (S0, term+1), if maj</span><br><span class="line">    (election timeout)                      (voting)</span><br><span class="line"></span><br><span class="line">Leader (S1, term)</span><br></pre></td></tr></table></figure>
<p>Election results:<br>
Each term has one leader at most despite failures (network partitions, server crashes).</p>
<ol>
<li>One leader is newly elected:</li>
</ol>
<ul>
<li>The leader sends heartbeat messages to followers in a heartbeat interval.</li>
<li>A server learns about newly elected leader from heartbeats with a higher term number in AppendEntries requests.</li>
</ul>
<ol start="2">
<li>Election failed for less than a maj of votes: New election starts with new term number. Old candidates quit.</li>
</ol>
<ul>
<li>Split votes</li>
<li>Not enough reachable servers</li>
</ul>
<ol start="3">
<li>Old leader holds hallucination that it is still a leader.</li>
</ol>
<p>Election rules:</p>
<ul>
<li>A peer stays follower if it hears from current leader with heartbeat message.</li>
<li>A peer starts leader election when it doesn’t receive the heartbeat message from current leader for an <code>election timeout</code>.</li>
<li>Each server can cast one vote per term.
<ul>
<li>A candidate will vote for itself.</li>
<li>A peer will vote for the first candidate that asks</li>
</ul>
</li>
<li>A candidate becomes a leader when it receives votes from a majority of peers for a given term.</li>
</ul>
<p>Problem 1.1: how does Raft avoid split votes?</p>
<p>Split votes can happen when simutaneous candidates ask for votes from a equal number of peers. Raft<br>
uses random election timeout to avoid this problem. The election timeout must be chosen wisely. It<br>
should be at least a few heartbeat intervals in case of network delays, long enough to elect a<br>
leader before the next election starts. It should also be short enough to allow retries and react to<br>
failures quickly.</p>
<h3><span id="log">Log</span><a href="#log" class="header-anchor">¶</a></h3>
<p>Keeping a log for old commands is common. But changing leader is the reason to have an ordered log which helps leader to check follower states.</p>
<p>Every server should hold an identical log. That means no server can execute a different command for a log entry that has been executed a command by any other server.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Log diagram:</span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">A leader (S2 or S3) for term 3 crashes before sending last AE to all.</span><br><span class="line"></span><br><span class="line">S1:  3           &lt;- term #</span><br><span class="line">S2:  3  3</span><br><span class="line">S3:  3  3</span><br><span class="line"></span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">Next election, S2 is elected and crashed for term 4.</span><br><span class="line">Next election, S3 is elected and crashed for term 5.</span><br><span class="line"></span><br><span class="line">    10 11 12 13  &lt;- log entry #</span><br><span class="line">S1:  3</span><br><span class="line">S2:  3  3  4</span><br><span class="line">S3:  3  3  5</span><br><span class="line"></span><br><span class="line">Leader (S3, 6)</span><br><span class="line"></span><br><span class="line">    10 11 12 13  &lt;- log entry #</span><br><span class="line">S1:  3</span><br><span class="line">S2:  3  3  4</span><br><span class="line">S3:  3  3  5  6</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><u>The first question is could this scenario arise and how.</u></p>
<p>Example 2: AE(log entry) details after Leader (S3, 6):</p>
<ol>
<li>S3 is chosen as new leader for term 6.
<ul>
<li>nextIndex[S2] = 13, nextIndex[S1] = 13</li>
</ul>
</li>
<li>It sends AE(13) with prevLogIndex=12, prevLogTerm=5</li>
<li>S2 replies false, mismatched prevLogTerm.</li>
<li>S3 decrements nextIndex[S2] to 12 (roll-back scheme, roll back one entry at a time)</li>
<li>S3 sends AE(12+13) with prevLogIndex=11, prevLogTerm=3</li>
<li>S2 deleted its entry 12<br>
The same for S1.</li>
</ol>
<p>The result of roll-back: followers hold the identical log with the leader by deleting the tail of log with mismatched term and accepting leader’s entries after that point.</p>
<p>It’s ok to forget about S2’s (12, 4) entry. It’s not received by any server yet and hence not committed. The client will resend the discarded commands of that term by and by.</p>
<p><strong>New leader would not roll back committed entries from end of previous term.</strong><br>
-&gt; Raft needs to ensure elected leader has all committed log entries.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Example 3:</span><br><span class="line"></span><br><span class="line">S1: 5 6 7</span><br><span class="line">S2: 5 8</span><br><span class="line">S3: 5 8</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Example 3: the longest log wins? -&gt; disclose voting details.</p>
<p>step 1: S2 or S3 wins election for term 5. It sends AEs to all.</p>
<p>step 2: S1 incurs network partition: S1 is isolated from S2 and S3.</p>
<p>step 3:</p>
<ul>
<li>S1: it doesnot hear from leader. Next election for term 6 starts. Can’t get maj and start new election for term 7…</li>
<li>S2 or S3: happily lives after S1 is disconnected since one of them will get maj and become leader.</li>
</ul>
<p>The problem is why won’t happy server choose 6 as next term?</p>
<p>Backing things up a litter, S1 is leader for term 6; crash + reboot; leader in term 7; crash and stay down. It crashes after it only sends AE to itself.</p>
<p>Because of leader (S1, 7), one of S2 or S3 learns about the last term is 7 while voting. So the next term will be 8.</p>
<p>All peers reboot. While S1 holds the longest log, it can’t be a leader since entry 8 could have committed.</p>
<p>end of section 5.4.1:<br>
<code>election restriction</code>, voters only cast votes for candidate who is <strong>at least as up-to-date</strong>.</p>
<ul>
<li>candidate has higher term in last log entry, or</li>
<li>candidate has same last term and same length or longer log</li>
</ul>
<p>The next leader will be S2 or S3. They vote for each other even if network connectivity is intact.<br>
Because they have higher term.</p>
<p>Then S1 will be forced to discard 6,7. -&gt; not committed -&gt; clients will resend the discarded commands</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Example 4:</span><br><span class="line"></span><br><span class="line">Leader (S2, 6); S1 crash+reboot; S2 sends AEs</span><br><span class="line"></span><br><span class="line">    Case 1      Case 2       Case 3</span><br><span class="line">S1: 4 5 5       4 4 4        4</span><br><span class="line">S2: 4 6 6 6 or  4 6 6 6  or  4 6 6 6</span><br></pre></td></tr></table></figure>
<p>end of section 5.3: a faster roll-back scheme than backs up one entry per request</p>
<p>How to roll back quickly?</p>
<p>S2: AE(prevLogTerm=6)<br>
-&gt; Reply: S1 rejects with (XTerm, XIndex, XLen)</p>
<ol>
<li>case 1, leader doesn’t have XTerm -&gt; nextIndex[S1] = XIndex</li>
<li>case 2, leader has XTerm -&gt; nextIndex[S1] = leader’s last entry for XTerm</li>
<li>case 3, follwer’s log too short -&gt; nextIndex[S1] = XLen</li>
</ol>
<p>(? binary search)</p>
<p>Phrases:<br>
XTerm: term in the conflicting entry (if any)<br>
XIndex: index of first entry with that term (if any)<br>
XLen: log length</p>
<h3><span id="persistence">Persistence</span><a href="#persistence" class="header-anchor">¶</a></h3>
<p>After a server crashes, we can repair it by replacing with a fresh (empty) server or reboot the crashed server.</p>
<ol>
<li>new: requires transfer of entire log/snapshots to new server, slow but necessary in case failure is permanent.</li>
<li>old: requires state that persists across crashes. Must support this for simultaneous power failures. -&gt; persistence</li>
</ol>
<p>What to remember?</p>
<ul>
<li>Essential info to let a Raft server rejoin the group. Save them after each change or before sending RPCs.</li>
<li>log[], a server that has latest log entries needs to keep that for furture use of new leader.</li>
<li>currentTerm, to ensure terms of system only increase. Each term has at most one leader to detect RPCs from stale leaders and candidates.</li>
<li>votedFor, a server needs to remember who it voted for in the currrent term in case it is rebooted and votes for a different candidate.</li>
</ul>
<p>What can be volatile?</p>
<p>If a state will be reset after reboot, it doesn’t need to be persistent.</p>
<p>Performance cost:</p>
<ul>
<li>Persistence is often the bottleneck for performance.
<ul>
<li>SSD (0.1ms per write) writes 100 times faster than a hard disk (10 ms per write), limiting us to 100-10,000 ops/sec.</li>
</ul>
</li>
<li>optimizations
<ul>
<li>batch many log entries per disk write</li>
<li>persist to faster storage (battery-backed RAM)</li>
</ul>
</li>
<li>Another bottleneck RPC takes &lt;&lt; 1 ms on a LAN</li>
</ul>
<p>How does the service recover its state after a crash+reboot?</p>
<ul>
<li>Simple approach: start with empty state and re-play Raft’s entire log.</li>
<li>Faster approach: snapshot + replay the tail of the log (that snapshot didn’t cover)</li>
</ul>
<h3><span id="log-compaction-and-snapshots">Log Compaction and Snapshots</span><a href="#log-compaction-and-snapshots" class="header-anchor">¶</a></h3>
<p>State = operation history</p>
<p>service states are usually much smaller than the complete log. A server can’t discard un-executed entries which is not yet reflected in the state and un-committed entries which could be part of leader’s majority.</p>
<p>To solve the problem of huge replicated log, service periodically creates persistent “snapshot”. A snapshot includes copy of service state and index of last included log entry. Service tells Raft it is snapshotted through some log index so Raft can discard log before that index. A server can create a snapshot and discard prefix of log whenever.</p>
<p>In the case of crash+restart, service reads snapshot from disk and tells Raft last included index to avoid duplicate applying of log entries.</p>
<p>When follower’s log ends before leader’s log starts, that part p is lost and can’t be recovered by AppendEntries RPCs. Instead leader sends InstallSnapshot RPCs.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">L:           |-p-|xxxxxxxxxxxxxx|</span><br><span class="line">F: |xxxxxxxxx|</span><br></pre></td></tr></table></figure>
<h3><span id="linearizability">Linearizability</span><a href="#linearizability" class="header-anchor">¶</a></h3>
<p>notion of correctness for strong consistency: linearizability</p>
<p>An execution history is linearizable if one can find a total order of all operations, that matches real-time (for non-overlapping ops), and in which each read sees the value from the write preceding it in the order.</p>
<p>A history is a record of client operations, each with arguments, return value, time of start, time completed.</p>
<p>draw the constraint arrows by time rule + value rule</p>
<ul>
<li>the order obeys value constraints (W -&gt; R)</li>
<li>the order obeys real-time constraints</li>
</ul>
<p>Notation:</p>
<ul>
<li>Wx1 means write value 1 to record x, Rx1 means a read of rocord x yielded value 1.</li>
<li>the history is client-centric view of requests. It is linearizable if there is no cycle</li>
<li>the form of overlapping operations denotes concurrent operations</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">example 4:</span><br><span class="line"></span><br><span class="line">C1: |-Wx3-|          |-Wx4-|</span><br><span class="line">C2:          |-Rx3-------------|</span><br><span class="line">order: Wx3 Rx3 Wx4</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>From the examples in the lecture, it is known that:</p>
<ol>
<li>service can pick either order for concurrent operations</li>
<li>all clients must see the writes in the same order expecially in the cases of replicas and caches</li>
<li>reads must return fresh data. Stale values aren’t linearizable</li>
<li>Duplicate request handling:
<ul>
<li>supposing clients re-send requests if they don’t get a reply, leader remembers client requests it has seen and replies with saved response from first execution when it sees duplicate.</li>
<li>it is linearizable to return the old saved value in this case. The reason is that the duplicate request is seen as the same process of the failed first request, which is concurrent to the write request.</li>
</ul>
</li>
</ol>
<h3><span id="duplicate-rpc-detection">Duplicate RPC Detection</span><a href="#duplicate-rpc-detection" class="header-anchor">¶</a></h3>
<p>Q: what should a client do if an operation times out? -&gt; Re-send th request</p>
<ul>
<li>server is dead or request dropped</li>
<li>server executed but request lost. Client still needs the result.</li>
</ul>
<p>The idea is duplicate RPC detection. Client picks an ID for each request. Re-sends will use the same ID in same RPC. Service keeps track a table indexed by ID and records value after executing. If the second RPC arrives with the same ID, the service knows it’s a duplicate and generates reply from the old saved value in the table.</p>
<p>Q: When can we delete table entries?</p>
<ul>
<li>If new leader takes over, how to get the duplicate table?
<ul>
<li>No need. Every server updates their duplicate table as they execute.</li>
</ul>
</li>
<li>If server crashes, how to restore its table?
<ul>
<li>replay of log populates the table or snapshots which contains a copy of the table</li>
<li>as discussed in linearizability section, it is allowed to return the old saved value in this case.</li>
</ul>
</li>
</ul>
<p>To keep the table small, service has one table entry per client rather than one per request. Each client numbers requests sequentially. When server receives a client request with new number, it can forget about client’s lower entries and update the table entry with the new request.</p>
<p>If a duplicate req arrives before the original executes, the service just call Start() again. Server won’t execute a req that is already seen.</p>
<h3><span id="read-only-operations">Read-only Operations</span><a href="#read-only-operations" class="header-anchor">¶</a></h3>
<p>end of section 8</p>
<p>Q: does the Raft leader have to commit read-only operations in the log before replying?</p>
<p>e.g. Get(key)?</p>
<p>Leader cannot respond immediately to a read-only operation using its current content. The reason is that a server might have lost the recent election but not realized it yet in cases of network partitions. Then a new leader processes Put()s for the key so that value in that server becomes stale. Thus leader must commit read-only operations.</p>
<p>But read-heavy workload are quite normal. Committing read-only operations can take a long time. In practice, people are often willing to exchange higher performance with stale data.</p>
<p>The idea is leases. Modify Raft as follows: defining a lease period, after each time the leader gets an AE maj, it is entitled to respond to read-only requests for a lease period without committing those requests to the log (w/o sending AEs).</p>
<p>A new leader can’t execute write requests until previous lease period has expired. So followers keep track of the last time they responded to an AE, and tell the new leader in the RequestVote reply.</p>
<p>As a result, we get faster read-only operations and the history is still linearizable.</p>
<h2><span id="paxos">Paxos</span><a href="#paxos" class="header-anchor">¶</a></h2>
<h3><span id="paxos-review">Paxos Review</span><a href="#paxos-review" class="header-anchor">¶</a></h3>
<p>Assumptions: non-Byzantine failures. Allows failures of missing messages and unordered requests.</p>
<p>P1. An acceptor must accept the first proposal that it receives.</p>
<p>For the acceptance of majority, one acceptor must accecpt &gt;= 1 proposals. Assign number to the proposal as (seq_number, value).</p>
<p>A value is chosen only when it is accepted by a majority of acceptors. -&gt; A value is chosen only when the proposal is chosen.</p>
<p>If multiple proposals are chosen, then it is guaranteed that all chosen proposals have the same value.</p>
<p>P2. If a proposal with value v is chosen, then every higher-numbered proposal <u>that is chosen</u> has value v.<br>
-&gt; 2a. accepted by any acceptor<br>
-&gt; 2b. issued by any proposer<br>
-&gt; 2c. For any v and n, if a proposal(n, v) is issued, then there is a set S consisting of a majority of acceptors such that either (a) no acceptor in S has accepted any proposal numbered less than n, or (b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the acceptors in S.</p>
<p>To prove 2c, the proposer (n, v) controls future acceptance by <strong>requesting a promise from acceptors that no more such acceptances of proposals numbered less than n</strong>.</p>
<p>Proposer’s algorithm:<br>
(1) a <code>prepare</code> req<br>
A proposer send a prososal (n)  and demands its acceptors to respond with:</p>
<ol>
<li>a promise never again to agree a proposal numbered less than n</li>
<li>the proposal with the highest number less than n that it has accepted (previous accepted value), if any.</li>
</ol>
<p>After the proposer gets the responses from a majority of the acceptors, it is getting into phase 2.</p>
<p>(2) an <code>accept</code> req, sharing the same set of acceptors as phase-1.<br>
The proposer issues a proposal (n, v) where v is the value either the highest-numbered proposal among the responses or the any one selected by the proposer if reported no proposals.</p>
<p>P1a. An acceptor can accept a proposal numbered n iff it has not responded to a prepare request having a number greater than n.</p>
<p>Phase 1:<br>
(1) proposer: sends a prepare req (n)<br>
(2) acceptor: responds the proposal(n, v) or null</p>
<p>Phase 2:<br>
(1) proposer: sends an accept req (n, v) to the same majority group of acceptors<br>
(2) acceptor: accepts (n, v) unless there are prepare requests numbered greater than n.</p>
<p>Optimizations:</p>
<ol>
<li>acceptors ignore the prepare requests numbered less than its accepted request number.</li>
<li>acceptors notify the proposers the highest number of its prepare requests so that proposers can drop the proposals with lower numbered req.</li>
</ol>
<p>Phase 3: learner: the acceptors can respond their acceptances to some set of distinguished learners.</p>
<p># of distinguished learners^, reliability^ &amp; communication complexity^.</p>
<p>Termination policy: a distinguished proposer must be selected as the only one to issue proposals. A reliable algorithm for electing a proposer must use either randomness or real time (timeouts).</p>
<p>implementation:</p>
<p>Paxos chooses a leader to perform the role of the distinguished proposer and the distinguished learner.</p>
<p>For persistence, an acceptor records its response in storage before sending. And each proposer keeps the highest-numbered proposal in storage before issuing.</p>
<blockquote class="colorquote blue"><p>Suppose that the acceptors are <em>A</em>, <em>B</em>, and <em>C</em>. <em>A</em> and <em>B</em> are also proposers. How does Paxos ensure that the following sequence of events can’t happen? What actually happens, and which value is ultimately chosen?</p>
<ol>
<li><em>A</em> sends prepare requests with proposal number 1, and gets responses from <em>A</em>, <em>B</em>, and <em>C</em>.</li>
<li><em>A</em> sends <code>accept(1, &quot;foo&quot;)</code> to <em>A</em> and <em>C</em> and gets responses from both. Because a majority accepted, <em>A</em> thinks that <code>&quot;foo&quot;</code> has been chosen. However, <em>A</em> crashes before sending an <code>accept</code> to <em>B</em>.</li>
<li><em>B</em> sends prepare messages with proposal number 2, and gets responses from <em>B</em> and <em>C</em>.</li>
<li><em>B</em> sends <code>accept(2, &quot;bar&quot;)</code> messages to <em>B</em> and <em>C</em> and gets responses from both, so <em>B</em> thinks that <code>&quot;bar&quot;</code> has been chosen.</li>
</ol>
</blockquote>
<p>After step 2, B and C have had the prepare message from A which is #1. After step 3, B would get response from C with value “foo” and gets null from B. At step 4, B will not send accecpt(2, “bar”) but (2, “foo”).</p>
<h3><span id="inference">Inference</span><a href="#inference" class="header-anchor">¶</a></h3>
<p>There are protocols such as Paxos, Raft, Viewstamped Replication, Raft, Zookeeper providing strong consistency. Paxos is different for its simplicity and wide application. Many other protocols can be viewed as variants of Paxos.</p>
<p>Two topics are:</p>
<p>Problem 1: How Paxos works?</p>
<p>Agreement is hard :(</p>
<ul>
<li>multiple rounds for the op, tentative initialy but don’t know when agreement is permanent</li>
<li>Agreement has to be able to complete even with failed servers. some servers decides a value but other servers don’t respond. Can’t distinguish between network partition (servers are running but cannot be reached by other partition) or failed servers.</li>
</ul>
<p>Two main ideas in Paxos to address these problems:</p>
<ul>
<li>many rounds are possible but they will converge on one value</li>
<li>majority rule: a key point is any two majorities overlap. At least one server with earlier majority is shared by the later majority.</li>
</ul>
<p>Paxos sketch:</p>
<ul>
<li>each server consists of three logical entities, acceptor, proposer, learner. Maybe more than one proposer when multiple clients submit requests to different servers at the same time.</li>
<li>Proposers contacts acceptors to assemble a majority. if not get a majority, new round.</li>
</ul>
<p>The crucial property:</p>
<ul>
<li>If a value was chosen, any subsequent choice must be the same value</li>
<li>chosen is system-wide property</li>
</ul>
<p>Why n?</p>
<ul>
<li>It distingushes among multiple rounds. Later rounds can supersede earlier rounds.</li>
<li>n = &lt;time, server_id&gt;, n must be unique and roughly follow time.</li>
<li>round(=proposal) numbers are WITHIN a particular instance.</li>
</ul>
<p>What’s the commit point?</p>
<p>i.e. a point that agreement has reached or a server has executed the op</p>
<p>After a majority has the same (v_a, n_a). Neither v_a nor n_a is sufficient.</p>
<p>Thinking (discussing with Claude):</p>
<ol>
<li>why does the proposer need to pick v_a with highest n_a?</li>
</ol>
<p>The proposer needs to pick an accepted value which is associated with the highest proposal number that acceptors have seen. It represents the most recent accepted value.</p>
<ol start="2">
<li>why does prepare handler check that n &gt; n_p?</li>
</ol>
<p>It picks a proposal number that is higher than current highest prepare seen. The proposer should not send a proposal that is older than previous ones.</p>
<ol start="3">
<li>why does accept handler check n &gt;= n_p?</li>
</ol>
<p>The acceptor promises not to accept any proposal with number less than n_p.</p>
<ol start="4">
<li>why does accept handler update n_p = n?</li>
</ol>
<p>The accept handler updates n_a because it’s accepting a value. The real question is why n_p is updated too. If n_p &lt; n, then n_p &lt; n_a. The proposal with n_p will be rejected by the acceptor.</p>
<ol start="5">
<li>what if proposer S2 chooses n &lt; S1’s n?</li>
</ol>
<p>It will be rejected by any acceptor that has seen S1’s proposal.</p>
<ol start="6">
<li>what if an acceptor crashes after receiving accept?</li>
</ol>
<p>Assume that an acceptor receiving accept means that states were updated but it didn’t send accept_ok, then the proposer will retry with a new proposal number and get rejected since the acceptor is dead.</p>
<p>One acceptor crash will not affect the the system as long as the majority of the paxos peers is alive. Since the paxos lab is not persistent, the peer will lost its states. It will get updated when it is alive.</p>
<ol start="7">
<li>what if an acceptor reboots after sending prepare_ok?</li>
</ol>
<p>The next accept will be rejected because the acceptor forgets the previous prepare which updates n_p.</p>
<p>Paxos gets stuck:</p>
<ul>
<li>not a majority that can communicate</li>
<li>propsers retry immediately after accept_reject (should retry after a random amount of time)</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">example 1 (normal operation):</span><br><span class="line">  S1, S2, S3</span><br><span class="line">  but S3 is dead or slow</span><br><span class="line">  S1 starts proposal, n=1 v=A</span><br><span class="line">S1: p1    a1A    dA</span><br><span class="line">S2: p1    a1A    dA</span><br><span class="line">S3: dead...</span><br><span class="line">&quot;p1&quot; means Sx receives prepare(n=1)</span><br><span class="line">&quot;a1A&quot; means Sx receives accept(n=1, v=A)</span><br><span class="line">&quot;dA&quot; means Sx receives decided(v=A)</span><br><span class="line">these diagrams are not specific about who the proposer is</span><br><span class="line">(I like the diagram in the lecture handout, very concise)</span><br><span class="line"></span><br><span class="line">proposer - acceptor exchange:</span><br><span class="line">    prepare(n) -&gt;</span><br><span class="line">    &lt;- prepare_ok(n, n_a, v_a)</span><br><span class="line">    accept(n, v&#x27;) -&gt;</span><br><span class="line">    &lt;- accept_ok(n)</span><br><span class="line">    decided(v&#x27;) -&gt;</span><br><span class="line"></span><br><span class="line">--- Paxos Proposer ---</span><br><span class="line"></span><br><span class="line">proposer(v):</span><br><span class="line">  while not decided:</span><br><span class="line">    choose n, unique and higher than any n seen so far</span><br><span class="line">    send prepare(n) to all servers including self</span><br><span class="line">    if prepare_ok(n, n_a, v_a) from majority:</span><br><span class="line">      v&#x27; = v_a with highest n_a; choose own v otherwise   </span><br><span class="line">      send accept(n, v&#x27;) to all</span><br><span class="line">      if accept_ok(n) from majority:</span><br><span class="line">        send decided(v&#x27;) to all</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--- Paxos Acceptor ---</span><br><span class="line"></span><br><span class="line">acceptor state on each node (persistent):</span><br><span class="line"> n_p     --- highest prepare seen</span><br><span class="line"> n_a, v_a --- highest accept seen</span><br><span class="line"></span><br><span class="line">acceptor&#x27;s prepare(n) handler:</span><br><span class="line"> if n &gt; n_p</span><br><span class="line">   n_p = n</span><br><span class="line">   reply prepare_ok(n, n_a, v_a)</span><br><span class="line"> else</span><br><span class="line">   reply prepare_reject</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">acceptor&#x27;s accept(n, v) handler:</span><br><span class="line"> if n &gt;= n_p</span><br><span class="line">   n_p = n</span><br><span class="line">   n_a = n</span><br><span class="line">   v_a = v</span><br><span class="line">   reply accept_ok(n)</span><br><span class="line"> else</span><br><span class="line">   reply accept_reject</span><br></pre></td></tr></table></figure>
<p><u>Problem 2: How to integrate Paxos into a real system?</u></p>
<p>Paxos-based replication:</p>
<p>Each client ops are appended to a replicated log of servers with a separate proposal number.<br>
Log entries are numbered, called instances (seq). Paxos reaches agreement on each log entry.</p>
<p>Paxos provides:</p>
<ul>
<li>interface:
<ul>
<li>Start (seq, v): Propose v for seq, without waiting for agreement to complete</li>
<li>fate, v := Status(seq): find out the agreed value for seq. Only check local states, w/o communications between servers.</li>
</ul>
</li>
<li>correctness: once any agreement reached, never changes. After agreement, servers may update state</li>
<li>fault tolerance: tolerate non-reachability of a minority of servers</li>
<li>liveness: will reach agreement if a majority of servers can communicate for long enough</li>
</ul>
<p>Example (context): Server Si uses Paxos to get all servers to agree that a log entry x holds a client op (Get,  Put, Append).</p>
<p>Why a log is better than all replicas to agree on each op in lock-step:</p>
<ul>
<li>when state is small: agree on entire state -&gt; a tie</li>
<li>when state is very large: log describes changes -&gt; win</li>
<li>log helps to recover from failures: slow, miss messages, crash, start</li>
</ul>
<p>Summary of how to use Paxos for RSM:</p>
<p>A log of Paxos instances (client op). Different instances’ Paxos agreements are <strong>independent</strong>.</p>
<h2><span id="case-study-zookeeper-2010">*Case Study: ZooKeeper 2010</span><a href="#case-study-zookeeper-2010" class="header-anchor">¶</a></h2>
<p>ZooKeeper: Wait-free coordination for Internet-scale systems</p>
<p>Read: 1-2</p>
<blockquote class="colorquote blue"><p>Problem 0: One use of Zookeeper is as a fault-tolerant lock service (see the section “Simple locks” on page 6). Why isn’t possible for two clients to acquire the same lock? In particular, how does Zookeeper decide if a client has failed and it can give the client’s locks to other clients?</p>
</blockquote>
<p>Because a lock can only be created once and held by one client. Once a lock is created, another client that tries to acquire the same lock will read the znode with the watch flag set. Zookeeper decides a client has failed by watch events.</p>
<p>&gt; Claude: The key safety guarantee comes from combining these mechanisms: locks are typically implemented using ephemeral znodes, and session expiration is what ultimately determines if a client has failed. A client’s lock (ephemeral znode) will only be released when its session expires, ensuring no two clients can hold the same lock simultaneously</p>
<p>Zookeeper is a Raft-like service with leader-based replication.  We care about Zookeeper from two perspectives:</p>
<ol>
<li>
<p>APIs for a general purpose  coordination service</p>
</li>
<li>
<p>performance of replication: will Nx replica server bring Nx performance?</p>
<p>Normally, it wouldn’t for leader-based replication. The server requires leader to replicate one by one, slowing down the performance inverse proportionally to the number of replicas.</p>
</li>
</ol>
<p>When a replica servers read-only client requests from their local state w/o other peers, reads from followers are not linearizable because they may return stale data. Even if a client is reading from an up-to-date replica, it still risks the possibility of seeing data values go backwards in time.</p>
<p>Zookeeper avoids this issue by changing the definition of correctness. It looses the constraints of linearizability by allowing reads to yield stale data so that clients are able to read from all replicas, changing total read capacity from O(1) to O(# servers). That highly increases the performance for read-heavy workloads.</p>
<p>Ordering guarantees:</p>
<ul>
<li>linearizable writes: writes are ordered with <code>zxid</code> and execute in zxid order.</li>
<li>FIFO client order</li>
</ul>
<img src="/sg/2023/07/28/ds_basics/zab.png" class title="zab">
<ol>
<li>
<p>API overview</p>
</li>
<li>
<p>atomicity for mini-transaction: Zookeeper is good for small piece of data like 100MB, not as big as 100GB.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># mini-transaction example</span><br><span class="line">while true:</span><br><span class="line">	x, v := getData(&quot;f&quot;)</span><br><span class="line">	if setData(x + 1, version=v):</span><br><span class="line">		break</span><br><span class="line">	(sleep)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>Scalable lock compared to threaded lock of Go:</p>
<p>: service creates sequential files. Each lock at most has one file requests to acquire. For example, supposing files are numbered with 0 - 3 and f0 holds the lock, the other files trying to acquire the lock look like the following diagram.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Locks w/o herd effect</span><br><span class="line">f0                   f1              f2          f3</span><br><span class="line">holds the lock &lt;-  wait for f0 &lt;- wait for f1 &lt;- wait for f2</span><br><span class="line"></span><br><span class="line"># Locks w/ herd effect</span><br><span class="line">f0                   f1              f2          f3</span><br><span class="line">holds the lock &lt;-  wait for f0   wait for f0   wait for f0</span><br><span class="line"> ^					 |                 |           |</span><br><span class="line"> |--------------------------------------------------</span><br></pre></td></tr></table></figure>
<h2><span id="case-study-chubby-2006">*Case Study: Chubby 2006</span><a href="#case-study-chubby-2006" class="header-anchor">¶</a></h2>
<p>The Chubby lock service for loosely-coupled distributed systems</p>
<p>Terms:</p>
<ol>
<li>Advisory locks are a type of locking mechanism where processes voluntarily check and respect locks, but the system doesn’t enforce the locks.</li>
<li>Whole-file reads refer to a pattern where applications read entire files from beginning to end, rather than accessing random portions or small parts of files. This was one of the key workload assumptions in systems like Google File System (GFS).</li>
</ol>
<p>This paper introduces Chubby, a distributed lock service, for use within a distributed system consisting of fairly large numbers of small machines connected by a high-speed network. It aims to deal with the problem of electing a leader from among a set of servers. The authors only want to discuss the design and implementation through real experience.</p>
<h2><span id="case-study-harp-1991">*Case Study: Harp 1991</span><a href="#case-study-harp-1991" class="header-anchor">¶</a></h2>
<p>Replication in the Harp File System</p>
<p>Read: 1, 3-6</p>
<blockquote class="colorquote blue"><p>Question: Figures 5-1, 5-2, and 5-3 show that Harp often finishes benchmarks faster than a conventional non-replicated NFS server. This may be surprising, since you might expect Harp to do strictly more work than a conventional NFS server (for example, Harp must manage the replication). Why is Harp often faster? Will all NFS operations be faster with Harp than on a conventional NFS server, or just some of them? Which?</p>
</blockquote>
<p>Harp is often faster because it applies three approaches to increase performance.</p>
<p>First, for non-modification operations, the primary node doesn’t send requests to backups and processes by local content.</p>
<p>Furthermore, since read operations in a file system are modifiled which changes the last viewed time of files upon reading, Harp makes a concession of consistency by allowing read requests to return to the client immediately from the primary. Such requests don’t have to wait for the acknowledgements of backups and increase performance. The rest of processing read requests between primary and backup is running at the background.</p>
<p>Secondly, a part of work is put background compared to unreplicated NFS server which is:</p>
<ol>
<li>in the phase 2 of 2PC, backups are informed about the commit</li>
<li>committed records of event are applies to file system (apply)</li>
</ol>
<p>Harp is a file service in a distributed network which was designed for small clusters. It adopts primary copy replication technique and write-ahead log, providing users similar interface to NFS at that time. Additional hardwares are used for reliable storage and UPS (uninterruptible power supply) which persists the log despite (short) power failures.</p>
<p>Environment: nodes are connected via a network; synchronized clocks allows a skew of &lt;100ms, node failures include network, hardware, power; a crash of node is fail-stop.</p>
<p>Problem: how replication works?</p>
<p>The structure of section 4 is: overview, system behavior of failure cases, no failure case (normal case processing), failure case (fail-over), atomicity of transactions</p>
<p>The things I noticed are:</p>
<ol>
<li>Some familiar stuff: Harp uses primary/back replication model + WAL (limited size; volatile; redo log, not undo) to achieve reliability, consistency, correctness, availability.
<ul>
<li>The replication models applies 2PC + view servers</li>
<li>It is fault-tolerant, allowing &lt;maj # of node failures</li>
<li>It is able to tolerate simultaneous failures.</li>
<li>reduce log size by exchanging and comparing LB and GLB.</li>
</ul>
</li>
<li>loosely synchronized clocks with a skew of &lt; 100ms.</li>
<li>Harp speeds up read op processing by using leases and sacrifice of a little consistency of last viewed time. When T(primary’s clock) &gt; T (backup’s clock) + t (promised by backup which won’t start a new view less than this time), the read requests are sent to backups. Otherwise, the primary returns the result of reads directly.</li>
<li>The member of Harp’s group has three roles: primary, backup, witness. Harp distributes work to all members by assigning roles with unmatched group. For example, in a system with three nodes A, B, C, A  could be primary of g0, backup of g1, and witness of g2.</li>
<li>Witness servers don’t store files because only (n+1) servers to store is enough for a system to tolerate n failure nodes. A witness server can be promoted and demoted.</li>
<li>A view of the system is numbered. The higher, the most recent. Harp avoids simultaneous view changes which slow down fail-over.</li>
</ol>
<p>Other things I wasn’t aware of:</p>
<ol>
<li>Harp uses Raft-like replication techniques, pre-dating Raft by 20+ years. It describes complete replicated systems and how it adapts to state machine abstraction.</li>
<li>question: how does it handle n/w partition for modified operations?</li>
<li>q: When can Harp form a new view?</li>
<li>Optimizations of Harp: pipelining of requests to backup (numbered event records?)</li>
</ol>
<p>Reference:</p>
<ol>
<li><a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2015/notes/l-paxos.txt">http://nil.csail.mit.edu/6.824/2015/notes/l-paxos.txt</a></li>
<li><a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2015/notes/paxos-code.html">http://nil.csail.mit.edu/6.824/2015/notes/paxos-code.html</a></li>
<li><a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2022/notes/l-raft.txt">http://nil.csail.mit.edu/6.824/2022/notes/l-raft.txt</a></li>
<li><a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2022/notes/l-raft2.txt">http://nil.csail.mit.edu/6.824/2022/notes/l-raft2.txt</a></li>
<li><a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2015/notes/l-harp.txt">http://nil.csail.mit.edu/6.824/2015/notes/l-harp.txt</a></li>
<li><a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/notes/l-zookeeper.txt">http://nil.csail.mit.edu/6.824/2020/notes/l-zookeeper.txt</a></li>
</ol>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/sg/">Home</a></li>
         
          <li><a href="/sg/about/">About</a></li>
         
          <li><a href="/sg/categories/">Category</a></li>
         
          <li><a href="/sg/archives/">Writing</a></li>
         
          <li><a href="/sg/CV/">CV</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Case Study: MapReduce 2004</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Consistency and Linearizability</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">Case Study: GFS 2003</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">Primary&#x2F;Backup Replication</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">Case Study: VMware FT 2010</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">5.1.</span> <span class="toc-text">FT design</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">5.2.</span> <span class="toc-text">Failure Handling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">5.3.</span> <span class="toc-text">Design Alternatives</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">Verified Primary&#x2F;Backup</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">Case Study: Grove 2023</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">8.</span> <span class="toc-text">Raft</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.1.</span> <span class="toc-text">Raft Review</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.2.</span> <span class="toc-text">Leader Election</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.3.</span> <span class="toc-text">Log</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.4.</span> <span class="toc-text">Persistence</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.5.</span> <span class="toc-text">Log Compaction and Snapshots</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.6.</span> <span class="toc-text">Linearizability</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.7.</span> <span class="toc-text">Duplicate RPC Detection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">8.8.</span> <span class="toc-text">Read-only Operations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">9.</span> <span class="toc-text">Paxos</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">9.1.</span> <span class="toc-text">Paxos Review</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">9.2.</span> <span class="toc-text">Inference</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">10.</span> <span class="toc-text">*Case Study: ZooKeeper 2010</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">11.</span> <span class="toc-text">*Case Study: Chubby 2006</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">12.</span> <span class="toc-text">*Case Study: Harp 1991</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&text=Distributed System Notes"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&title=Distributed System Notes"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&is_video=false&description=Distributed System Notes"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Distributed System Notes&body=Check out this article: https://sgzerolc.github.io/sg/2023/07/28/ds_basics/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&title=Distributed System Notes"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&title=Distributed System Notes"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&title=Distributed System Notes"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&title=Distributed System Notes"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&name=Distributed System Notes&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://sgzerolc.github.io/sg/2023/07/28/ds_basics/&t=Distributed System Notes"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2025
    Sam Li
  </div>
</footer>


    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/sg/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
